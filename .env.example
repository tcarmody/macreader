# LLM Provider Configuration
# Set at least one API key for AI features (summarization, topic clustering)

# Anthropic Claude (recommended - supports prompt caching for 90% cost reduction)
ANTHROPIC_API_KEY=your-anthropic-key-here

# OpenAI GPT (alternative)
# OPENAI_API_KEY=your-openai-key-here

# Google Gemini (alternative)
# GOOGLE_API_KEY=your-google-key-here

# Preferred provider: "anthropic", "openai", or "google"
# If not set, uses first available key in order: Anthropic > OpenAI > Google
# LLM_PROVIDER=anthropic

# Optional: override the default model for your provider
# Anthropic: claude-haiku-4-5, claude-sonnet-4-5, claude-opus-4-5
# OpenAI: gpt-5.2-mini, gpt-5.2
# Google: gemini-3.0-flash, gemini-3.0-pro
# LLM_MODEL=

# Server Configuration
PORT=5005
LOG_LEVEL=INFO
CACHE_DIR=./data/cache
DB_PATH=./data/articles.db

# Rate limiting
API_RPM_LIMIT=50
